# -*- coding: utf-8 -*-
"""Untitled3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1LHC6fCMaJgTrEzoYLMHqbammZTec-mrG
"""

import numpy as np
import matplotlib.pyplot as plt
from numpy import linalg as LA
from matplotlib.patches import Ellipse
from sklearn.datasets.samples_generator import make_blobs
from scipy.stats import multivariate_normal
K = 3
NUMDATAPTS = 150
X, y = make_blobs(n_samples=NUMDATAPTS, centers=K, shuffle=False, random_state=0, cluster_std =0.6)
g1 = np . asarray ( [ [ 2.0 , 0 ] , [ -0.9 , 1 ] ] )
g2 = np . asarray ( [ [ 1.4 , 0 ] , [ 0.5 , 0.7 ] ] )
mean1 = np .mean(X[ : int (NUMDATAPTS/K) ] )
mean2 = np .mean(X[ int (NUMDATAPTS/K):2*int(NUMDATAPTS/K) ] )
X[ : int (NUMDATAPTS/K) ] = np . einsum( 'nj,ij->ni' ,
X[ : int (NUMDATAPTS/K) ] - mean1 , g1 ) + mean1
X[ int (NUMDATAPTS/K):2*int (NUMDATAPTS/K) ] = np . einsum( 'nj,ij->ni' ,
X[ int (NUMDATAPTS/K):2*int (NUMDATAPTS/K) ] - mean2 , g2 ) + mean2
X[ : , 1 ] -= 4

'Part (a)'
mu = np.random.rand(K,2) #initial mean
cov = np.zeros((K,2,2)) #initial cov
for i in range(K):
    cov[i] = np.eye(2)
m =np.ones(K)/K
pi = m/np.sum(m) #uniform distribution

# print(mu, mu.shape)
# print(cov, cov.shape)
# print(pi, pi.shape)

'Part (b)'
def E_step():
    gamma=np.zeros((NUMDATAPTS,K))
    for i in range(NUMDATAPTS):
        for j in range(K):
            gamma[i,j] = pi[j]*multivariate_normal.pdf(X[i],mu[j],cov[j])
        gamma[i,:] /= np.sum(gamma[i,:])
    return gamma

'Part (c)'
def M_step(gamma):
    for i in range(K):
        count = np.sum(gamma[:,i], axis=0)
        pi[i] = count/NUMDATAPTS
        mu[i] = (np.sum(X*gamma[:,i].reshape(NUMDATAPTS,1), axis=0))/count
        cov[i] = np.dot((np.array(gamma[:,i]).reshape(NUMDATAPTS,1)*(X-mu[i])).T, (X-mu[i]))/count
    return mu, pi, cov

'Part (d)'
def plot_result(gamma=None):
    ax = plt.subplot(111, aspect='equal')
    ax.set_xlim([-5,5])
    ax.set_ylim([-5,5])
    ax.scatter(X[:, 0], X[:, 1], c=gamma, s=50, cmap=None)
    
    for k in range(K):
        l, v = LA.eig(cov[k])
        theta = np.arctan(v[1,0]/v[0,0])
        e = Ellipse((mu[k, 0], mu[k, 1]), 6*l[0], 6*l[1], theta*180/np.pi)
        e.set_alpha(0.5)
        ax.add_artist(e)
        
    plt.show()

threshold = 1  # edit threshold here
likelihood1 = float('-inf')
print('Threshold: ', threshold)
while True:
    likelihood0 = likelihood1
    likelihood1 = 0
    e = E_step()
    mu, pi, cov = M_step(e)
    for i in range(NUMDATAPTS):
        sum = 0
        for j in range(K):
            sum += pi[j]*multivariate_normal.pdf(X[i],mu[j],cov[j])
        likelihood1 += np.log(sum)
    plot_result(e)
    print("Likelihood: ", likelihood1)
    print("Diff: ", likelihood1-likelihood0)
    if (likelihood1 - likelihood0) < threshold:
        print('Satisfied threshold of ', threshold)
        break

from sklearn.cluster import KMeans
kmeans = KMeans(n_clusters=K, random_state=0).fit(X)
Z = kmeans.predict(X)

plt.scatter(X[:, 0], X[:, 1], c=Z, s=50, cmap='viridis')

centers = kmeans.cluster_centers_
plt.scatter(centers[:, 0], centers[:, 1], c='black', s=200, alpha=0.5)

import numpy as np
from sklearn import decomposition
from sklearn import datasets

X = datasets.load_diabetes().data

from sklearn.preprocessing import scale
scaled_X = scale(X)
U, S, V = np.linalg.svd(scaled_X)
print('V matrix' + '\n', V)
print("Singular values" + '\n', S)

print("3 most important components: ")
pca = decomposition.PCA(n_components=3)
comp = pca.fit_transform(scaled_X)
print(comp[:10])

